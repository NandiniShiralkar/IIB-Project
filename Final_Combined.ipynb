{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f8dc8c48580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from GSM._imports import *\n",
    "from torch.optim import Adam\n",
    "from SSN._imports import *\n",
    "from SSN.params import GridParameters\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FoGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoGSMModel(nn.Module):\n",
    "    def __init__(self,thetas=None, l_feature=1.0, l_amplitude=1.2, kappa=1.0, jitter=1e-5, grid_size=10, frequency=1.0,sigma=0.1):\n",
    "        super(FoGSMModel, self).__init__()\n",
    "\n",
    "        self.dtype = torch.float64\n",
    "\n",
    "        self.l_feature = Parameter(torch.tensor(l_feature, dtype=self.dtype))\n",
    "        self.l_amplitude = Parameter(torch.tensor(l_amplitude, dtype=self.dtype))\n",
    "        self.kappa = Parameter(torch.tensor(kappa, dtype=self.dtype))\n",
    "        self.frequency = Parameter(torch.tensor(frequency, dtype=self.dtype)) \n",
    "        self.sigma = Parameter(torch.tensor(sigma))\n",
    "\n",
    "        if thetas is None:\n",
    "            thetas = torch.linspace(0, 2 * np.pi, 8)  # 8 orientations from 0 to 2*pi\n",
    "        self.thetas = thetas\n",
    "\n",
    "        self.jitter = jitter\n",
    "        self.grid_size = grid_size\n",
    "        self.grid = torch.stack(torch.meshgrid(torch.linspace(-5, 5, grid_size), \n",
    "                                               torch.linspace(-5, 5, grid_size)), \n",
    "                                dim=-1).reshape(-1, 2)\n",
    "        \n",
    "        self.K_g = self.generate_K_g()\n",
    "        \n",
    "    def von_mises_kernel(self, theta1, theta2):\n",
    "        theta_diff = theta1 - theta2  \n",
    "        return torch.clamp(torch.exp(self.kappa * torch.cos(theta_diff)), min=1e-6)\n",
    "    \n",
    "    def squared_exponential_kernel(self, x1, x2, length_scale,jitter=\"True\"):\n",
    "        x1 = x1.unsqueeze(1) # Shape: [N, 1, 2]\n",
    "        x2 = x2.unsqueeze(0) # Shape: [1, N, 2]\n",
    "        sq_dist = torch.sum((x1 - x2) ** 2, dim=2) # Shape: [N, N]\n",
    "\n",
    "        exp_term = torch.exp(-sq_dist / (2*length_scale**2))\n",
    "\n",
    "        if jitter:\n",
    "            return exp_term + self.jitter * torch.eye(x1.size(0))\n",
    "        else:\n",
    "            return exp_term\n",
    "\n",
    "    def composite_feature_kernel(self, theta1, theta2):\n",
    "         \n",
    "        sq_exp_component = self.squared_exponential_kernel(self.grid, self.grid, self.l_feature,jitter=\"False\")        \n",
    "        \n",
    "        # Ensure theta1 and theta2 are tensors\n",
    "        theta1 = torch.tensor(theta1).clone().detach().requires_grad_(True)\n",
    "        theta2 = torch.tensor(theta2).clone().detach().requires_grad_(True)\n",
    "\n",
    "        x1 = self.grid.unsqueeze(1) # Shape: [N, 1, 2]\n",
    "        x2 = self.grid.unsqueeze(0) # Shape: [1, N, 2]\n",
    "\n",
    "        n1 = torch.tensor([torch.cos(theta1), torch.sin(theta1)]).view(1, 1, 2)  # Shape: [1, 1, 2]\n",
    "        n2 = torch.tensor([torch.cos(theta2), torch.sin(theta2)]).view(1, 1, 2)  # Shape: [1, 1, 2]\n",
    "        average_orientation = (n1 + n2) / 2\n",
    "\n",
    "        # Broadcasting average_orientation for dot product computation\n",
    "        average_orientation = average_orientation.repeat(x1.size(0), x1.size(1), 1)\n",
    "        dot_product = torch.sum((x1 - x2) * average_orientation, dim=2)\n",
    "        periodic_component = torch.cos(2 * torch.pi * self.frequency * dot_product)\n",
    "\n",
    "        # Composite Kernel\n",
    "        return sq_exp_component * periodic_component\n",
    "\n",
    "    def generate_K_g(self):\n",
    "        \n",
    "        theta1_grid, theta2_grid = torch.meshgrid(self.thetas, self.thetas)\n",
    "        ori_kernel_val = self.von_mises_kernel(theta1_grid, theta2_grid)\n",
    "    \n",
    "        # Spatial kernel\n",
    "        loc_kernel_val = torch.zeros((len(self.thetas), len(self.thetas), self.grid_size**2, self.grid_size**2))\n",
    "\n",
    "        for i in range(len(self.thetas)):\n",
    "            for j in range(len(self.thetas)):\n",
    "                loc_kernel_val[i,j] = self.composite_feature_kernel(self.thetas[i], self.thetas[j])\n",
    "        \n",
    "        K_spatial = torch.sum(loc_kernel_val, dim=[0, 1])\n",
    "        K_g = torch.kron(K_spatial, ori_kernel_val)\n",
    "        \n",
    "        K_g = K_g + self.jitter * torch.eye(len(self.thetas)*self.grid_size**2)\n",
    "        return K_g\n",
    "\n",
    "    def compute_A(self):\n",
    "        kernel_vals = self.squared_exponential_kernel(self.grid, self.grid, self.l_amplitude)\n",
    "        return torch.sqrt(torch.exp(MultivariateNormal(torch.zeros(self.grid.size(0)), kernel_vals).sample()))\n",
    "\n",
    "    def samples(self):\n",
    "\n",
    "        g = MultivariateNormal(torch.zeros(len(self.thetas)*(self.grid_size**2)), self.K_g).sample()  \n",
    "        A = self.compute_A()\n",
    "        \n",
    "        # Tile amplitudes to match feature fields \n",
    "        A = A.repeat(len(self.thetas))\n",
    "    \n",
    "        # Combine\n",
    "        I = g * A  + torch.randn_like(g) * self.sigma\n",
    "        #I = torch.sum(I.reshape(len(self.thetas), self.grid_size, self.grid_size), dim=0)\n",
    "\n",
    "        return I, g\n",
    "    \n",
    "    def log_likelihood(self, I, g,A):\n",
    "        A = A.repeat(len(self.thetas))\n",
    "        I_hat = g * A\n",
    "        #I_hat = torch.sum(I_hat.reshape(len(self.thetas), self.grid_size, self.grid_size), dim=0)\n",
    "        print(\"Log likelihood I_hat \",I_hat.size())\n",
    "        print(\"Log likelihood I \",I.size())\n",
    "        return MultivariateNormal(I_hat.flatten(), self.sigma * torch.eye(self.grid_size**2)).log_prob(I.flatten())\n",
    "\n",
    "    def likelihood(self, I, g,A):\n",
    "        A = A.repeat(len(self.thetas))\n",
    "        I_hat = g * A     \n",
    "\n",
    "        # Ensure positive definite covariance matrix\n",
    "        cov_matrix = self.sigma * torch.eye(I_hat.size(0))\n",
    "        if not torch.isfinite(cov_matrix).all():\n",
    "            print(\"Non-finite values in covariance matrix\")\n",
    "            return torch.tensor(float('nan'))\n",
    "        \n",
    "        return torch.exp(MultivariateNormal(I_hat, self.sigma * torch.eye(I_hat.size(0))).log_prob(I))\n",
    "\n",
    "    def visualise(self, combined_fields):\n",
    "\n",
    "        combined_fields = torch.sum(combined_fields.reshape(len(self.thetas), self.grid_size, self.grid_size), dim=0)\n",
    "\n",
    "        # Normalise the combined image for visualisation\n",
    "        combined_fields_normalised = combined_fields / combined_fields.max()\n",
    "\n",
    "        # Reshape to image format\n",
    "        combined_image = combined_fields_normalised.view(self.grid_size, self.grid_size).detach().numpy()\n",
    "\n",
    "        # Visualise the combined image\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.imshow(combined_image, cmap='gray') \n",
    "        plt.title('FoGSM Sample')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    def generate_fogsm_dataset(self, num_samples, save=False,save_path=None):\n",
    "        samples = []\n",
    "        for _ in range(num_samples):\n",
    "            I, g = self.samples()\n",
    "            samples.append((I, g))\n",
    "    \n",
    "        # Convert samples to tensors\n",
    "        images, gs = zip(*samples)\n",
    "        images = torch.stack(images)\n",
    "        gs = torch.stack(gs)\n",
    "    \n",
    "        if save:\n",
    "            torch.save(images, gs, save_path)\n",
    "        else:\n",
    "            return images\n",
    "\n",
    "    def visualise_samples(self, save_path, num_samples_to_visualise, grid_size):\n",
    "        # Load the saved samples\n",
    "        images, _ = torch.load(save_path)\n",
    "\n",
    "        # Select a subset of samples to visualise\n",
    "        selected_samples = images[:num_samples_to_visualise]\n",
    "\n",
    "        # Create a grid of images\n",
    "        fig, axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
    "        fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < num_samples_to_visualise:\n",
    "                image = selected_samples[i].detach().numpy()\n",
    "                image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "                ax.imshow(image, cmap='gray')\n",
    "        \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_18419/3850355274.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta1 = torch.tensor(theta1).clone().detach().requires_grad_(True)\n",
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_18419/3850355274.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta2 = torch.tensor(theta2).clone().detach().requires_grad_(True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "num_oris = 8\n",
    "thetas = torch.linspace(0, torch.pi, num_oris)  # 8 orientations from 0 to pi\n",
    "fogsm_params = {\n",
    "        \"thetas\": thetas,\n",
    "        \"l_feature\": 1.0,\n",
    "        \"l_amplitude\": 1.5,\n",
    "        \"kappa\": .5,\n",
    "        \"jitter\": 1e-5,\n",
    "        \"grid_size\": 3,\n",
    "        \"frequency\": .2,\n",
    "        \"sigma\": 0.1,\n",
    "    }\n",
    "fogsm_model = FoGSMModel(**fogsm_params)\n",
    "\n",
    "# Generate and visualise a sample from the FoGSM model\n",
    "I, g = fogsm_model.samples()\n",
    "#fogsm_model.visualise(I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSN Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSNBase(torch.nn.Module):\n",
    "    def __init__(self, n, k, Ne, Ni, tau_e, tau_i, device='cpu', dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.Ne = Ne\n",
    "        self.Ni = Ni\n",
    "        self.N = self.Ne + self.Ni\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.register_buffer('EI', torch.cat([torch.ones(Ne), torch.zeros(Ni)]).to(device).bool())\n",
    "        self.register_buffer('tau_vec', torch.cat([tau_e * torch.ones(Ne), tau_i * torch.ones(Ni)]).to(device, dtype))\n",
    "\n",
    "        self.C_E = nn.Parameter(torch.tensor(2.4)) # taken from Echeveste et al. 2020\n",
    "        self.C_I = nn.Parameter(torch.tensor(2.4)) # taken from Echeveste et al. 2020\n",
    "\n",
    "        b = 13.24\n",
    "        self.beta = nn.Parameter(torch.tensor(1.5 * b))\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.6)) # taken from Echeveste et al. 2020\n",
    "    \n",
    "    def drdt(self, r, inp_vec):\n",
    "        return (-r + self.powlaw(self.W @ r + inp_vec)) / self.tau_vec\n",
    "\n",
    "    def powlaw(self, u):\n",
    "        return self.k * F.relu(u).pow(self.n)\n",
    "    \n",
    "    def dvdt_batch(self, v, inp_vec):\n",
    "        # Ensure v and inp_vec are 2D tensors for batch processing\n",
    "        if v.ndim == 1:\n",
    "            v = v.unsqueeze(0)\n",
    "        if inp_vec.ndim == 1:\n",
    "            inp_vec = inp_vec.unsqueeze(0)\n",
    "\n",
    "        #print(f\"dvdt: v.shape={v.shape}, inp_vec.shape={inp_vec.shape}, W.shape={self.W.shape}\")\n",
    "\n",
    "        # Compute W @ r for batch processing\n",
    "        W_r = self.W @ self.powlaw(v).T\n",
    "        W_r = W_r.T  # Transpose back to match batch dimension\n",
    "\n",
    "        return (-v + W_r + inp_vec) / self.tau_vec\n",
    "    \n",
    "    def process_input(self, input_batch):\n",
    "\n",
    "        # Non-linearity for the input to the network\n",
    "        input_rectified = F.relu(self.beta + input_batch)\n",
    "        epsilon = 1e-6  # Small value to avoid zero\n",
    "        input_processed = (input_rectified + epsilon) ** self.gamma\n",
    "\n",
    "        # Multiply the input batch by C_E and C_I scalars\n",
    "        input_weighted = torch.cat([\n",
    "            self.C_E * input_processed,\n",
    "            self.C_I * input_processed\n",
    "        ], dim=-1)\n",
    "\n",
    "        return input_weighted\n",
    "\n",
    "    def simulate_batch_rates(self, inp_vec, v_init=None, duration=500, dt=1,noise=None):\n",
    "        # inp_vec: [batch_size, num_neurons]\n",
    "        # v_init: [batch_size, num_neurons]\n",
    "        # returns: [batch_size, time_steps, num_neurons]\n",
    "        \n",
    "        time_steps = int(duration/dt)\n",
    "        print(f\"simulate: time_steps={time_steps}, dt={dt}, duration={duration}\")\n",
    "        # Check if inp_vec is a batch of inputs\n",
    "        if inp_vec.ndim == 2:\n",
    "            batch_size, self.N = inp_vec.shape \n",
    "            inp_vec = inp_vec.unsqueeze(1).expand(-1, time_steps, -1)\n",
    "        else:\n",
    "            raise ValueError(\"inp_vec must be a 2D tensor of shape [batch_size, num_neurons].\")\n",
    "\n",
    "        #print(f\"simulate: batch_size={batch_size}, time_steps={time_steps}, inp_vec.shape={inp_vec.shape}\")\n",
    "\n",
    "        # Initialise v_init if not provided\n",
    "        if v_init is None:\n",
    "            v_init = torch.zeros((batch_size, self.N), device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            if isinstance(v_init, (int, float)):\n",
    "                v_init = torch.full((batch_size, self.N), v_init, device=self.device, dtype=self.dtype)\n",
    "            if v_init.shape[0] != batch_size or v_init.shape[1] != self.N:\n",
    "                raise ValueError(\"v_init shape does not match batch_size or neuron count N.\")\n",
    "        \n",
    "        # Initialise noise if not provided\n",
    "        if noise is None:\n",
    "            noise = torch.zeros((time_steps, self.N), device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            if noise.shape[0] != time_steps or noise.shape[1] != self.N:\n",
    "                raise ValueError(\"noise shape does not match time_steps or neuron count N.\")\n",
    "\n",
    "        # rates: [batch_size, time_steps, Ne]\n",
    "        rates_E = torch.zeros((batch_size, time_steps, self.Ne), device=self.device, dtype=self.dtype)\n",
    "        v = v_init\n",
    "        t = dt\n",
    "        while t < duration:\n",
    "            # Calculate dv for each element in the batch at the current time step\n",
    "            dv = self.dvdt_batch(v, inp_vec[:, t, :]+noise[t])  # dv: [batch_size, num_neurons]\n",
    "\n",
    "            # Update the membrane potentials for the current time step\n",
    "            v += dt * dv\n",
    "\n",
    "            # take an average of membrane potentials over trials at each time step is constant over time\n",
    "            # but trials doesny ecist so instead average over neurons but not image\n",
    "\n",
    "            # TODO: throw away the first 100 ms burn-in time of the simulation to allow the network to settle to a stable state before recording the firing rates\n",
    "\n",
    "            # Compute the firing rates from the updated membrane potentials\n",
    "            r = self.powlaw(v) # r: [batch_size, num_neurons]\n",
    "\n",
    "            # Store the rates for the current time step\n",
    "            if t % 5 == 0:\n",
    "                rates_E[:, t, :] = r[:, :self.Ne] # rates: [batch_size, time_steps, Ne]\n",
    "\n",
    "            t = t + dt\n",
    "\n",
    "        return rates_E\n",
    "    \n",
    "    def simulate_batch(self, inp_vec, v_init=None, duration=500, dt=1,burn_in_time=100,noise=None):\n",
    "        # inp_vec: [batch_size, num_neurons]\n",
    "        # v_init: [batch_size, num_neurons]\n",
    "        # returns: [batch_size, time_steps, num_neurons]\n",
    "        \n",
    "        time_steps = int(duration/dt)\n",
    "\n",
    "        print(f\"simulate: time_steps={time_steps}, dt={dt}, duration={duration}\")\n",
    "\n",
    "        if inp_vec.ndim == 2:\n",
    "            batch_size, self.N = inp_vec.shape \n",
    "            inp_vec = inp_vec.unsqueeze(1).expand(-1, time_steps, -1)\n",
    "        else:\n",
    "            raise ValueError(\"inp_vec must be a 2D tensor of shape [batch_size, num_neurons].\")\n",
    "        \n",
    "        # Initialise v_init if not provided\n",
    "        if v_init is None:\n",
    "            v_init = torch.zeros((batch_size, self.N), device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            if isinstance(v_init, (int, float)):\n",
    "                v_init = torch.full((batch_size, self.N), v_init, device=self.device, dtype=self.dtype)\n",
    "            if v_init.shape[0] != batch_size or v_init.shape[1] != self.N:\n",
    "                raise ValueError(\"v_init shape does not match batch_size or neuron count N.\")\n",
    "        \n",
    "        # Initialise noise if not provided\n",
    "        if noise is None:\n",
    "            noise = torch.zeros((time_steps, self.N), device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            if noise.shape[0] != time_steps or noise.shape[1] != self.N:\n",
    "                raise ValueError(\"noise shape does not match time_steps or neuron count N.\")\n",
    "\n",
    "        # rates: [batch_size, time_steps, Ne]\n",
    "        potentials_E = torch.zeros((batch_size, time_steps, self.Ne), device=self.device, dtype=self.dtype)\n",
    "        v = v_init\n",
    "        t = 0\n",
    "\n",
    "        while t < duration:\n",
    "            # Calculate dv for each element in the batch at the current time step\n",
    "            dv = self.dvdt_batch(v, inp_vec[:, t, :]+noise[t])  # dv: [batch_size, num_neurons]\n",
    "\n",
    "            # Update the membrane potentials for the current time step\n",
    "            v += dt * dv # v: [batch_size, num_neurons]\n",
    "\n",
    "            # Store every 5th time step\n",
    "            if t >= burn_in_time:\n",
    "                # Store every 5th time step after the burn-in period\n",
    "                if (t - burn_in_time) % 5 == 0:\n",
    "                    potentials_E[:, t, :] = v[:, :self.Ne] # rates: [batch_size, time_steps, Ne]\n",
    "\n",
    "            t = t + dt\n",
    "\n",
    "        return potentials_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSN 2DTopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSN2DTopo(SSNBase):\n",
    "    def __init__(self, n, k, tauE, tauI, grid_pars, conn_pars, thetas, L = np.pi,device='cpu', dtype=torch.float64):\n",
    "         \n",
    "        num_oris = thetas.shape[0]\n",
    "        grid_size = grid_pars['grid_size_Nx']\n",
    "                \n",
    "        Ne = num_oris * (grid_size ** 2)\n",
    "        Ni = num_oris * (grid_size ** 2)\n",
    "\n",
    "        super(SSN2DTopo, self).__init__(n=n, k=k, Ne=Ne, Ni=Ni, tau_e=tauE, tau_i=tauI, device=device, dtype=dtype)\n",
    "        \n",
    "        self.num_oris = num_oris\n",
    "        self.grid_size = grid_size\n",
    "        self.Ne = Ne\n",
    "        self.Ni = Ni\n",
    "        self.L = L\n",
    "        self.thetas = thetas\n",
    "        self._make_maps()\n",
    "        \n",
    "        # Initialise trainable parameters\n",
    "        if conn_pars is None:\n",
    "            conn_pars = {\n",
    "                'J_2x2': torch.rand(2, 2, device=device, dtype=dtype),  \n",
    "                's_2x2': torch.rand(2, 2, device=device, dtype=dtype),  \n",
    "                'p_local': torch.rand(2, device=device, dtype=dtype),  \n",
    "                'sigma_oris': torch.rand(1, device=device, dtype=dtype)  \n",
    "            }\n",
    "            \n",
    "        self.J_2x2 = nn.Parameter(conn_pars['J_2x2'])  # Interaction strengths\n",
    "        self.s_2x2 = nn.Parameter(conn_pars['s_2x2']) # Spatial length scales\n",
    "        self.p_local = nn.Parameter(conn_pars['p_local']) # Local connectivity strengths\n",
    "        self.sigma_oris = nn.Parameter(conn_pars['sigma_oris']) # Orientation tuning width\n",
    "        \n",
    "        self.make_W()\n",
    "\n",
    "    def _make_maps(self):\n",
    "\n",
    "        self.ori_map = torch.tensor(self.thetas, device=self.device, dtype=self.dtype).flatten()\n",
    "        self.ori_vec = self.ori_map.repeat(self.grid_size ** 2)  # Repeat orientation values for each grid cell\n",
    "        self.ori_vec = self.ori_vec.repeat(2) # Repeat for E and I populations\n",
    "\n",
    "        # Create x and y vectors for grid cells\n",
    "        self.x_vec = torch.arange(self.grid_size, device=self.device, dtype=self.dtype).repeat_interleave(self.num_oris).repeat(self.grid_size)\n",
    "        self.y_vec = torch.arange(self.grid_size, device=self.device, dtype=self.dtype).repeat_interleave(self.num_oris * self.grid_size)\n",
    "\n",
    "        # Repeat for E and I populations\n",
    "        self.x_vec = self.x_vec.repeat(2)\n",
    "        self.y_vec = self.y_vec.repeat(2)\n",
    "    \n",
    "    def calc_xy_dist(self):\n",
    "        Ne = Ni = self.Ne\n",
    "        x_vec_e = self.x_vec[:Ne]\n",
    "        y_vec_e = self.y_vec[:Ne]\n",
    "        x_vec_i = self.x_vec[Ne:Ne+Ni]\n",
    "        y_vec_i = self.y_vec[Ne:Ne+Ni]\n",
    "        \n",
    "        xy_dist = torch.cdist(torch.stack([x_vec_e, y_vec_e], dim=1), torch.stack([x_vec_i, y_vec_i], dim=1), p=2).repeat(2, 2) \n",
    "\n",
    "        return xy_dist\n",
    "    \n",
    "    def calc_ori_dist(self,L=np.pi, method=None):\n",
    "\n",
    "        Ne = self.num_oris * self.grid_size ** 2\n",
    "        ori_vec = self.ori_vec[:Ne]\n",
    "\n",
    "        if method == \"absolute\":\n",
    "            ori_dist = torch.cdist(ori_vec.unsqueeze(1), ori_vec.unsqueeze(1)).repeat(2,2)\n",
    "        else:\n",
    "            #1 - cos(2(pi/L) * |theta1 - theta2|^2)\n",
    "            ori_diff = torch.abs(ori_vec.unsqueeze(1) - ori_vec.unsqueeze(0))\n",
    "            ori_diff = torch.min(ori_diff, 2*np.pi - ori_diff)\n",
    "            sq_ori_dist = 1 - torch.cos((2*np.pi / L) * ori_diff**2)\n",
    "                    \n",
    "        sq_ori_dist = sq_ori_dist.repeat(2, 2)\n",
    "\n",
    "        return sq_ori_dist\n",
    "\n",
    "    def make_W(self):\n",
    "        \n",
    "        xy_dist = self.calc_xy_dist()\n",
    "        sq_ori_dist = self.calc_ori_dist()\n",
    "\n",
    "        # Compute weight blocks\n",
    "        W_ee = self.calc_W_block(xy_dist[:self.Ne, :self.Ne], sq_ori_dist[:self.Ne, :self.Ne], self.s_2x2[0][0], self.sigma_oris, excitatory=True, p_loc=self.p_local[0])\n",
    "        W_ei = self.calc_W_block(xy_dist[:self.Ne, self.Ne:], sq_ori_dist[:self.Ne, self.Ne:], self.s_2x2[0][1], self.sigma_oris, excitatory=True, p_loc=self.p_local[1])\n",
    "        W_ie = self.calc_W_block(xy_dist[self.Ne:, :self.Ne], sq_ori_dist[self.Ne:, :self.Ne], self.s_2x2[1][0], self.sigma_oris)\n",
    "        W_ii = self.calc_W_block(xy_dist[self.Ne:, self.Ne:], sq_ori_dist[self.Ne:, self.Ne:], self.s_2x2[1][1], self.sigma_oris)\n",
    "        \n",
    "        # Apply local connectivity strengths\n",
    "        W_ee = self.p_local[0] * torch.eye(self.Ne, device=self.device, dtype=self.dtype) + (1 - self.p_local[0]) * W_ee\n",
    "        W_ei = self.p_local[1] * torch.eye(self.Ni, device=self.device, dtype=self.dtype) + (1 - self.p_local[1]) * W_ei\n",
    "                \n",
    "        # Concatenate submatrices to form W\n",
    "        W = torch.cat([\n",
    "            torch.cat([self.J_2x2[0, 0] * W_ee, self.J_2x2[0, 1] * W_ei], dim=1).clone().detach(),\n",
    "            torch.cat([self.J_2x2[1, 0] * W_ie, self.J_2x2[1, 1] * W_ii], dim=1).clone().detach()\n",
    "        ], dim=0).double()\n",
    "\n",
    "        # Register W as a buffer\n",
    "        self.register_buffer('W', W)\n",
    "        \n",
    "        return self.W\n",
    "\n",
    "    def calc_W_block(self, xy_dist, sq_ori_dist, s, sigma_oris, CellWiseNormalised = True, excitatory=False,p_loc=None):\n",
    "\n",
    "        #Add a small constant to s and sigma_oris to avoid division by zero\n",
    "        s = s + 1e-8\n",
    "        sigma_oris = 2*np.pi*sigma_oris/self.L + 1e-8\n",
    "        \n",
    "        if excitatory:\n",
    "            W_spatial =  torch.exp(-xy_dist / s) \n",
    "            W_orientation = torch.exp(- sq_ori_dist / (2 * sigma_oris ** 2))\n",
    "            W = (p_loc * torch.eye(W_spatial.size(0)) + (1 - p_loc) * W_spatial) * W_orientation\n",
    "        else:\n",
    "            W =  torch.exp(-xy_dist ** 2 / (2 * s ** 2) - sq_ori_dist / (2 * sigma_oris ** 2))\n",
    "                               \n",
    "        W = torch.where(W < 1e-4, torch.zeros_like(W), W)\n",
    "\n",
    "        sW = torch.sum(W, dim=1, keepdim=True)\n",
    "        if CellWiseNormalised:\n",
    "            W = torch.div(W, sW)\n",
    "        else:\n",
    "            sW = sW.mean()\n",
    "            W = W / sW\n",
    "\n",
    "        return W.squeeze()\n",
    "    \n",
    "    def make_correlated_noise(self, dt, Nt, corr_time, grid_size=3):\n",
    "\n",
    "        # Generate temporally correlated noise\n",
    "        Tfilt = 10 * corr_time\n",
    "        Nfilt = int(Tfilt / dt)\n",
    "        ttfilt = torch.arange(0, Nfilt+1,dtype=torch.float32) * dt\n",
    "        assert Nt > len(ttfilt)\n",
    "        filter = torch.exp(-ttfilt / corr_time) * dt / corr_time\n",
    "\n",
    "        white_noise = torch.randn(self.N, Nt + Nfilt) / torch.sqrt(torch.tensor(dt)) * torch.sqrt(torch.tensor(2 * corr_time))\n",
    "\n",
    "        noise = []\n",
    "        for nn in range(self.N):\n",
    "            coloured = torch.conv1d(white_noise[nn:nn+1, :].unsqueeze(0), filter.unsqueeze(0).unsqueeze(0), padding='valid').squeeze()\n",
    "            noise.append(coloured)\n",
    "        noise = torch.stack(noise)\n",
    "\n",
    "        # params taken from Echeveste et al. 2020 - optimised values\n",
    "        noise_var_EI=torch.tensor([6.399186720550809504,3.533514014946008697]) \n",
    "        noise_std_EI = torch.sqrt(noise_var_EI)\n",
    "        rho = 0.992852418877574472 \n",
    "        d_sigma = 0.477952645032425183\n",
    "\n",
    "        # Introduce spatial correlations\n",
    "        sigma_e = noise_std_EI[0]\n",
    "        sigma_i = noise_std_EI[1]\n",
    "        \n",
    "        theta_unsq = self.thetas.unsqueeze(1)  \n",
    "        theta_unsq_T = self.thetas.unsqueeze(0)  \n",
    "\n",
    "        sigma_ee = sigma_e**2 * torch.exp((torch.cos(2 * (theta_unsq - theta_unsq_T)) - 1) / d_sigma**2)\n",
    "        sigma_ii = sigma_i**2 * torch.exp((torch.cos(2 * (theta_unsq - theta_unsq_T)) - 1) / d_sigma**2)\n",
    "        sigma_ei = rho * sigma_e * sigma_i * torch.exp((torch.cos(2 * (theta_unsq - theta_unsq_T)) - 1) / d_sigma**2)\n",
    "        sigma_ie = rho * sigma_e * sigma_i * torch.exp((torch.cos(2 * (theta_unsq_T - theta_unsq)) - 1) / d_sigma**2)\n",
    "\n",
    "        blocks = [\n",
    "            torch.kron(torch.eye(grid_size**2), sigma_ee),\n",
    "            torch.kron(torch.eye(grid_size**2), sigma_ei),\n",
    "            torch.kron(torch.eye(grid_size**2), sigma_ie),\n",
    "            torch.kron(torch.eye(grid_size**2), sigma_ii)\n",
    "        ]\n",
    "\n",
    "        sigma = torch.cat([\n",
    "            torch.cat([blocks[0], blocks[1]], dim=1),\n",
    "            torch.cat([blocks[2], blocks[3]], dim=1)\n",
    "        ], dim=0).double()\n",
    "\n",
    "        sigma = sigma + 1e-6 * torch.eye(sigma.size(0))\n",
    "        cholesky = torch.linalg.cholesky(sigma)\n",
    "\n",
    "        # set dtype to float64\n",
    "        noise = noise.double()\n",
    "        cholesky = cholesky.double()\n",
    "\n",
    "        correlated_noise = cholesky @ noise\n",
    "\n",
    "        return correlated_noise.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSN Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1635.5401, dtype=torch.float64)\n",
      "tensor([[ 15.2398, -47.1603],\n",
      "        [ 47.5999, -39.9800]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "k = 0.3\n",
    "tauE = 20.0\n",
    "tauI = 10.0\n",
    "grid_pars = {'grid_size_Nx': 3}\n",
    "\n",
    "psi = torch.tensor(7.774)\n",
    "conn_pars = {\n",
    "    'J_2x2': torch.tensor([[.624, -1.931], [1.949, -1.637]],dtype=torch.float64)*psi*torch.pi,\n",
    "    's_2x2': torch.tensor([[0.2955, 0.09], [0.5542, 0.09]])/0.4,  \n",
    "    'p_local': torch.tensor([0.72,0.7]),  \n",
    "    'sigma_oris': torch.tensor(45.0),\n",
    "    'num_oris': 8}\n",
    "\n",
    "# print j_EEj_II - j_EIj_IE - torch.tensor([[.624, -1.931], [1.049, -0.537]]\n",
    "print(conn_pars['J_2x2'][0,0]*conn_pars['J_2x2'][1,1] - conn_pars['J_2x2'][0,1]*conn_pars['J_2x2'][1,0])\n",
    "print(conn_pars['J_2x2'])\n",
    "\n",
    "thetas = torch.linspace(0, torch.pi, conn_pars['num_oris'])\n",
    "\n",
    "ssn_params = {\n",
    "        \"n\": 2,\n",
    "        \"k\": k,\n",
    "        \"tauE\": 20,\n",
    "        \"tauI\": 10,\n",
    "        \"grid_pars\": grid_pars,\n",
    "        \"conn_pars\": conn_pars,\n",
    "        \"thetas\": thetas,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_18419/2855143407.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ori_map = torch.tensor(self.thetas, device=self.device, dtype=self.dtype).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "simulate: time_steps=500, dt=1, duration=500\n",
      "Rate of neuron  0  is too high to plot\n",
      "Rate of neuron  1  is too high to plot\n",
      "Rate of neuron  2  is too high to plot\n",
      "Rate of neuron  3  is too high to plot\n",
      "Rate of neuron  4  is too high to plot\n",
      "Rate of neuron  5  is too high to plot\n",
      "Rate of neuron  6  is too high to plot\n",
      "Rate of neuron  7  is too high to plot\n",
      "Rate of neuron  8  is too high to plot\n",
      "Rate of neuron  9  is too high to plot\n",
      "Rate of neuron  10  is too high to plot\n",
      "Rate of neuron  11  is too high to plot\n",
      "Rate of neuron  12  is too high to plot\n",
      "Rate of neuron  13  is too high to plot\n",
      "Rate of neuron  14  is too high to plot\n",
      "Rate of neuron  15  is too high to plot\n",
      "Rate of neuron  16  is too high to plot\n",
      "Rate of neuron  17  is too high to plot\n",
      "Rate of neuron  18  is too high to plot\n",
      "Rate of neuron  19  is too high to plot\n",
      "Rate of neuron  20  is too high to plot\n",
      "Rate of neuron  21  is too high to plot\n",
      "Rate of neuron  22  is too high to plot\n",
      "Rate of neuron  23  is too high to plot\n",
      "Rate of neuron  24  is too high to plot\n",
      "Rate of neuron  25  is too high to plot\n",
      "Rate of neuron  26  is too high to plot\n",
      "Rate of neuron  27  is too high to plot\n",
      "Rate of neuron  28  is too high to plot\n",
      "Rate of neuron  29  is too high to plot\n",
      "Rate of neuron  30  is too high to plot\n",
      "Rate of neuron  31  is too high to plot\n",
      "Rate of neuron  32  is too high to plot\n",
      "Rate of neuron  33  is too high to plot\n",
      "Rate of neuron  34  is too high to plot\n",
      "Rate of neuron  35  is too high to plot\n",
      "Rate of neuron  36  is too high to plot\n",
      "Rate of neuron  37  is too high to plot\n",
      "Rate of neuron  38  is too high to plot\n",
      "Rate of neuron  39  is too high to plot\n",
      "Rate of neuron  40  is too high to plot\n",
      "Rate of neuron  41  is too high to plot\n",
      "Rate of neuron  42  is too high to plot\n",
      "Rate of neuron  43  is too high to plot\n",
      "Rate of neuron  44  is too high to plot\n",
      "Rate of neuron  45  is too high to plot\n",
      "Rate of neuron  46  is too high to plot\n",
      "Rate of neuron  47  is too high to plot\n",
      "Rate of neuron  48  is too high to plot\n",
      "Rate of neuron  49  is too high to plot\n",
      "Rate of neuron  50  is too high to plot\n",
      "Rate of neuron  51  is too high to plot\n",
      "Rate of neuron  52  is too high to plot\n",
      "Rate of neuron  53  is too high to plot\n",
      "Rate of neuron  54  is too high to plot\n",
      "Rate of neuron  55  is too high to plot\n",
      "Rate of neuron  56  is too high to plot\n",
      "Rate of neuron  57  is too high to plot\n",
      "Rate of neuron  58  is too high to plot\n",
      "Rate of neuron  59  is too high to plot\n",
      "Rate of neuron  60  is too high to plot\n",
      "Rate of neuron  61  is too high to plot\n",
      "Rate of neuron  62  is too high to plot\n",
      "Rate of neuron  63  is too high to plot\n",
      "Rate of neuron  64  is too high to plot\n",
      "Rate of neuron  65  is too high to plot\n",
      "Rate of neuron  66  is too high to plot\n",
      "Rate of neuron  67  is too high to plot\n",
      "Rate of neuron  68  is too high to plot\n",
      "Rate of neuron  69  is too high to plot\n",
      "Rate of neuron  70  is too high to plot\n",
      "Rate of neuron  71  is too high to plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# produce a fogsm sample\n",
    "I, g = fogsm_model.samples()\n",
    "\n",
    "# simulate the SSN model\n",
    "ssn_model = SSN2DTopo(**ssn_params)\n",
    "\n",
    "# process the input\n",
    "inp_vec = ssn_model.process_input(I)\n",
    "inp_vec = inp_vec.unsqueeze(0)\n",
    "\n",
    "# create correlated noise\n",
    "dt = 1\n",
    "Nt = 500\n",
    "corr_time = 20\n",
    "noise = ssn_model.make_correlated_noise(dt, Nt, corr_time)\n",
    "\n",
    "# simulate the SSN model\n",
    "potentials = ssn_model.simulate_batch(inp_vec, duration=500, dt=1,noise=noise)\n",
    "rates = ssn_model.powlaw(potentials)\n",
    "\n",
    "# plot the rate of ith neuron over time\n",
    "for i in range (ssn_model.Ne):\n",
    "    if rates[0,:,i].max() < 10000:\n",
    "        plt.plot(rates[0,:,i].detach().numpy())\n",
    "    else:\n",
    "        print(\"Rate of neuron \",i,\" is too high to plot\")\n",
    "plt.title(\"Rate of ith neuron over time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirectFit:\n",
    "    def __init__(self, ssn_params, fogsm_params):\n",
    "        \n",
    "        super(DirectFit, self).__init__()\n",
    "        self.ssn_model = SSN2DTopo(**ssn_params)\n",
    "        self.fogsm_model = FoGSMModel(**fogsm_params)\n",
    "        self.lambda_0 = None\n",
    "\n",
    "    def sample_trajectories(self, input_batch, duration=500, dt=.2):\n",
    "\n",
    "        input_weighted = self.ssn_model.process_input(input_batch)\n",
    "\n",
    "        # Ensure input_reshaped has correct dimensions before calling simulate\n",
    "        if input_weighted.shape[1] != self.ssn_model.N:\n",
    "            input_weighted = input_weighted.view(input_weighted.shape[0], self.ssn_model.N)\n",
    "        \n",
    "        # Geenrate noise\n",
    "        noise = self.ssn_model.make_correlated_noise(dt=dt, Nt=int(duration/dt),corr_time=0.5,grid_size=3)\n",
    "            \n",
    "        # Generate trajectories for the reshaped input\n",
    "        potentials = self.ssn_model.simulate_batch(input_weighted, duration=duration, dt=dt,noise=noise)\n",
    "        trajectories = self.ssn_model.powlaw(potentials)\n",
    "\n",
    "        return trajectories,potentials\n",
    "\n",
    "    def calculate_log_p_g(self, trajectory):\n",
    "        # trajectory: [num_samples_g, dim]\n",
    "        # expectation over g in Eq (25) (1st term), i.e., (sub-sampled) time-steps of SSN simulation, corresponding to samples from variational posterior of g\n",
    "\n",
    "        num_neurons = trajectory.shape[1]  # Number of neurons\n",
    "        dim = trajectory.shape[0]            # Number of time points\n",
    "\n",
    "        # Mean vector for the multivariate normal (zero mean)\n",
    "        mean = torch.zeros(num_neurons, device=trajectory.device, dtype=trajectory.dtype)\n",
    "    \n",
    "        # Covariance matrix for the multivariate normal\n",
    "        cov = self.fogsm_model.K_g\n",
    "\n",
    "        # Check covariance matrix shape\n",
    "        assert cov.shape == (num_neurons, num_neurons), f\"Expected covariance matrix of shape ({num_neurons}, {num_neurons}), but got {cov.shape}\"\n",
    "\n",
    "        # Multivariate normal distribution\n",
    "        mvg = MultivariateNormal(mean, cov)\n",
    "\n",
    "        # Calculate log probabilities for all samples in the trajectory\n",
    "        log_probs = []\n",
    "        for g in trajectory:\n",
    "            if torch.isnan(g).any() or torch.isinf(g).any():\n",
    "                print(\"Invalid values in g:\", g)\n",
    "            log_probs.append(mvg.log_prob(g))  # log_probs: [num_samples_g]\n",
    "\n",
    "        log_probs = torch.tensor(log_probs, device=trajectory.device, dtype=trajectory.dtype)\n",
    "        if torch.isnan(log_probs).any():\n",
    "            print(\"NaN detected in log_probs\")\n",
    "\n",
    "        # Return the mean log probability\n",
    "        log_p_g = log_probs.mean()\n",
    "\n",
    "        return log_p_g\n",
    "\n",
    "    def calculate_log_p_I_given_g(self, I_data, trajectory, A_samples, epsilon=1e-6):\n",
    "\n",
    "        log_likelihood = 0\n",
    "\n",
    "        # expectation over g in Eq (25) (2nd term)\n",
    "        for g in trajectory: # runs over (sub-sampled) time-steps of SSN simulation, corresponding to samples from variational posterior of g\n",
    "            p_I_g = 0\n",
    "\n",
    "            # expectation over A in Eq (25)\n",
    "            for a in A_samples: # for computational cost reasons, currently using the same samples for A for all images in a batch (but we resample across mini-batches) \n",
    "                p_I_g = p_I_g + self.fogsm_model.likelihood(I_data, g, a) # I_data is a single image from the dataset and g is a single sample\n",
    "                                                                          # for g from the SSN\n",
    "                #check if p_I_g is zero or negative\n",
    "                if p_I_g < 0:\n",
    "                    print(\"p_I_g is zero\")\n",
    "\n",
    "            avg_p_I_g = p_I_g / len(A_samples)\n",
    "            avg_p_I_g = torch.clamp(avg_p_I_g, min=epsilon) # to avoid log(0)\n",
    "\n",
    "            log_likelihood = log_likelihood + torch.log(avg_p_I_g)\n",
    "        \n",
    "        return log_likelihood / len(trajectory)\n",
    "\n",
    "    def calc_lambda_0(self, elbo_batches, reg_term):\n",
    "        # assuming elbo_batches & reg_term are both scalars\n",
    "        # and reg_term is the output of regularisation_term function for lambda_0 = lambda_factor = 1\n",
    "        self.lambda_0 = elbo_batches / reg_term\n",
    "\n",
    "        return self.lambda_0\n",
    "    \n",
    "    def regularisation_term(self, potentials, elbo_image, skip_steps=10, lambda_factor=1.0, option='b'):\n",
    "        # potentials: [batch_size, time_steps, num_neurons]\n",
    "\n",
    "        # Calculate the average of squared potentials over neurons\n",
    "        v_mean = torch.mean(potentials**2, dim=-1) # shape: [batch_size, time_steps]\n",
    "        v_mean = v_mean[:, ::skip_steps]\n",
    "        if torch.isnan(v_mean).any():\n",
    "            print(\"NaN detected in v_mean\")\n",
    "\n",
    "        print(\"v_mean: \", v_mean)\n",
    "        # Check if v_mean is constant\n",
    "        if torch.all(v_mean == v_mean[0, :]):\n",
    "            print(\"v_mean is constant over time steps\")\n",
    "\n",
    "        if option == 'a':\n",
    "            # Option a) Taking the absolute difference of nearest neighbor time steps\n",
    "            diff_a = torch.diff(v_mean, dim=1)\n",
    "            \n",
    "            reg_term = torch.mean(diff_a)\n",
    "        else:\n",
    "            # Option b) Taking the difference across all pairs of time steps\n",
    "            diff_b = torch.abs(v_mean.unsqueeze(1) - v_mean.unsqueeze(2)) \n",
    "            print(\"diff_b:\", diff_b)\n",
    "\n",
    "            if torch.isnan(diff_b).any():\n",
    "                print(\"NaN detected in diff_b\")\n",
    "            reg_term = torch.mean(diff_b) \n",
    "\n",
    "        if self.lambda_0 is None:\n",
    "            lambda_0 = self.calc_lambda_0(elbo_image, reg_term)\n",
    "        #lambda_0 = self.lambda_0 if lambda_0 is None else lambda_0\n",
    "\n",
    "        if lambda_factor is None: \n",
    "            print(\"NaN detected in lambda_factor\")\n",
    "        if self.lambda_0 is None:\n",
    "            print(\"NaN detected in lambda_0\")\n",
    "\n",
    "        print(\"lambda_0: \", self.lambda_0)\n",
    "        print(\"lambda_factor: \", lambda_factor)\n",
    "        print(\"reg_term: \", reg_term)\n",
    "        reg_term = lambda_factor * self.lambda_0 * reg_term\n",
    "        if torch.isnan(reg_term).any():\n",
    "            print(\"NaN detected in reg_term\")\n",
    "\n",
    "        return reg_term\n",
    "\n",
    "    def calculate_elbo(self, input_batch, A_samples, duration=500, dt=.2):\n",
    "\n",
    "        # Sample trajectories from the SSN\n",
    "        trajectories,potentials = self.sample_trajectories(input_batch=input_batch, duration=duration, dt=dt)\n",
    "\n",
    "        elbo = 0\n",
    "        for I, trajectory in zip(input_batch, trajectories): # trajectory is the trajectory for a single image\n",
    "            log_p_g = self.calculate_log_p_g(trajectory)\n",
    "            log_p_I_given_g = self.calculate_log_p_I_given_g(I, trajectory, A_samples)\n",
    "\n",
    "            cov_matrix = torch.cov(trajectory.T)\n",
    "\n",
    "            if not torch.isfinite(cov_matrix).all():\n",
    "                print(\"Non-finite values in covariance matrix\")\n",
    "            \n",
    "            if torch.isnan(cov_matrix).any():\n",
    "                print(\"NaN detected in cov_matrix\")\n",
    "            \n",
    "            cov_matrix = cov_matrix + 1e-6 * torch.eye(cov_matrix.size(0))\n",
    "            entropy_term = 0.5 * torch.logdet(cov_matrix)\n",
    " \n",
    "            elbo_image = log_p_g + log_p_I_given_g + entropy_term \n",
    "            reg_term = self.regularisation_term(potentials=potentials,elbo_image=elbo_image)\n",
    "            \n",
    "            print(\"ELBO terms: \", log_p_g, log_p_I_given_g, entropy_term,reg_term)\n",
    "\n",
    "            elbo = elbo + elbo_image + reg_term\n",
    "\n",
    "        elbo_batches = elbo / len(input_batch)\n",
    "        return elbo_batches\n",
    "        \n",
    "    def optimise_elbo(self, batch_size, num_samples_a, threshold=1e-3, optimizer_cls=Adam, lr=1e-3,duration=10, dt=0.2):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.ssn_model.to(device)\n",
    "        self.fogsm_model.to(device)\n",
    "\n",
    "        optimizer = optimizer_cls(list(self.ssn_model.parameters()), lr=lr)\n",
    "        batch = 0\n",
    "        prev_elbo = float('-inf')\n",
    "        elbo_values = []  \n",
    "\n",
    "        while True:\n",
    "            \n",
    "            optimizer.zero_grad()  # Zero gradients at the beginning of each batch\n",
    "\n",
    "            elbo_batch = 0\n",
    "\n",
    "            # Sample amplitude fields from the prior p(A) using Monte Carlo sampling\n",
    "            A_samples = [self.fogsm_model.compute_A() for _ in range(num_samples_a)]\n",
    "\n",
    "            # Generate a mini-batch of input data\n",
    "            input_batch = self.fogsm_model.generate_fogsm_dataset(batch_size).to(device)\n",
    "\n",
    "            # Calculate the ELBO for the mini-batch\n",
    "            elbo_batch = self.calculate_elbo(input_batch, A_samples, duration=duration, dt=dt)\n",
    "\n",
    "            if torch.isnan(elbo_batch).any():\n",
    "                print(\"NaN detected in elbo_batch\")\n",
    "                break\n",
    "\n",
    "            # Optimise the ELBO with respect to the model parameters\n",
    "            elbo_batch.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print the ELBO for monitoring\n",
    "            if (batch + 1) % 1 == 0:\n",
    "                print(f\"Batch [{batch+1}], ELBO: {elbo_batch / batch_size:.4f}\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if abs(elbo_batch - prev_elbo) < threshold:\n",
    "                print(f\"Converged after {batch+1} batches.\")\n",
    "                break\n",
    "\n",
    "            prev_elbo = elbo_batch\n",
    "            batch = batch + 1\n",
    "            elbo_values.append(elbo_batch.item())\n",
    "\n",
    "            if batch > 100:\n",
    "                # plot the elbo values\n",
    "                plt.plot(elbo_values)\n",
    "                plt.xlabel(\"Batch\")\n",
    "                plt.ylabel(\"ELBO\")\n",
    "                plt.title(\"ELBO values\")\n",
    "                plt.show()\n",
    "\n",
    "                break\n",
    "        \n",
    "        return self.ssn_model, self.fogsm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_18419/2855143407.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ori_map = torch.tensor(self.thetas, device=self.device, dtype=self.dtype).flatten()\n",
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_18419/3850355274.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta1 = torch.tensor(theta1).clone().detach().requires_grad_(True)\n",
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_18419/3850355274.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta2 = torch.tensor(theta2).clone().detach().requires_grad_(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "simulate: time_steps=100, dt=1, duration=100\n",
      "v_mean:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "v_mean is constant over time steps\n",
      "diff_b: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)\n",
      "lambda_0:  tensor(-inf, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "lambda_factor:  1.0\n",
      "reg_term:  tensor(0., dtype=torch.float64)\n",
      "NaN detected in reg_term\n",
      "ELBO terms:  tensor(-94.5970, dtype=torch.float64) tensor(-13.8155, grad_fn=<DivBackward0>) tensor(-497.3584, dtype=torch.float64) tensor(nan, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "v_mean:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "v_mean is constant over time steps\n",
      "diff_b: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)\n",
      "lambda_0:  tensor(-inf, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "lambda_factor:  1.0\n",
      "reg_term:  tensor(0., dtype=torch.float64)\n",
      "NaN detected in reg_term\n",
      "ELBO terms:  tensor(-94.5970, dtype=torch.float64) tensor(-13.8155, grad_fn=<DivBackward0>) tensor(-497.3584, dtype=torch.float64) tensor(nan, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "v_mean:  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "v_mean is constant over time steps\n",
      "diff_b: tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=torch.float64)\n",
      "lambda_0:  tensor(-inf, dtype=torch.float64, grad_fn=<DivBackward0>)\n",
      "lambda_factor:  1.0\n",
      "reg_term:  tensor(0., dtype=torch.float64)\n",
      "NaN detected in reg_term\n",
      "ELBO terms:  tensor(-94.5970, dtype=torch.float64) tensor(-13.8155, grad_fn=<DivBackward0>) tensor(-497.3584, dtype=torch.float64) tensor(nan, dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "NaN detected in elbo_batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(SSN2DTopo(), FoGSMModel())"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direct_fit = DirectFit(ssn_params, fogsm_params)\n",
    "direct_fit.optimise_elbo(batch_size=3, num_samples_a=10, optimizer_cls=Adam, lr=0.1, duration=100, dt=1) #TODO: grab samples every 5ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "simulate: time_steps=500, dt=1, duration=500\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ssn_model = direct_fit.ssn_model\n",
    "fogsm_model = direct_fit.fogsm_model\n",
    "\n",
    "# Generate a sample from the FoGSM model\n",
    "I, g = fogsm_model.samples()\n",
    "I = I.unsqueeze(0)\n",
    "\n",
    "# Process the input\n",
    "input_weighted = ssn_model.process_input(I)\n",
    "\n",
    "# Generate correlated noise\n",
    "dt = 1\n",
    "Nt = 500\n",
    "corr_time = 20\n",
    "noise = ssn_model.make_correlated_noise(dt, Nt, corr_time)\n",
    "\n",
    "# Simulate the SSN model\n",
    "potentials = ssn_model.simulate_batch(input_weighted, duration=500, dt=1,noise=noise)\n",
    "rates = ssn_model.powlaw(potentials)\n",
    "\n",
    "# Plot the rate of ith neuron over time\n",
    "for i in range(ssn_model.Ne):\n",
    "    if rates[0,:,i].max() < 10000:\n",
    "        plt.plot(rates[0,:,i].detach().numpy())\n",
    "    else:\n",
    "        print(\"Rate of neuron \",i,\" is too high to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_83470/3445253102.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta1 = torch.tensor(theta1).clone().detach().requires_grad_(True)\n",
      "/var/folders/l7/fy1r11ts4_3988dhm6lgmr2w0000gn/T/ipykernel_83470/3445253102.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta2 = torch.tensor(theta2).clone().detach().requires_grad_(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of images:  tensor(13.2183, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the standard deviation of FoGSM images\n",
    "\n",
    "fogsm_model = FoGSMModel(**fogsm_params)\n",
    "num_samples = 10000\n",
    "samples = []\n",
    "for _ in range(num_samples):\n",
    "    I, g = fogsm_model.samples()\n",
    "    samples.append((I, g))\n",
    "images, _ = zip(*samples)\n",
    "images = torch.stack(images)\n",
    "sd = images.std()\n",
    "print(\"Standard deviation of images: \", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
